#+title: WIP Using Deep Learning to Assist Modelling of Tokyo Real Estate Prices
#+SUBTITLE: WIP An evaluation of an incorporation of deep learning methods for prediction real estate prices
#+AUTHOR: Jiyan Jonas Schneider
#+EMAIL:     jiyan.schneider@gmail.com
#+DATE:      2021-12-20
#+LATEX_HEADER: \usepackage[backend=biber, style=apa,]{biblatex}
#+LATEX_HEADER: \usepackage{xeCJK}
#+BIBLIOGRAPHY: /Users/jiyanschneider/Dropbox/Documents/lib/bibliography/bibliography.bib
# #+LATEX_HEADER: \setCJKmainfont{HiraginoSans-W0}
#+LATEX_HEADER: \setmainfont{EBGaramond-Regular}
#+latex_class_options: [12pt]
#+LATEX_HEADER: \usepackage[a4paper,left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
#+OPTIONS: toc:nil

#+begin_abstract

The aim of this study is to examine the structure of real estate rent prices, taking
into account the layout of the property. We find that when using computer vision methods
to analyze the apartment rent
The aim of this study is to examine the viability of using Deep Learning methods
and what they might have in housing. There have been quite a few studies
investigating the use of Machine Learning methods for this kind of task. The
novelty in our study primarily lies in the use of Computer Vision Deep Learning
methods to analyze the images of the floor plans to assist with prediction.
By using a computer vision models described in [cite:@park2015using] to help with
assistance we find that making use of the computer vision model for prediction can
improve accuracy by ...  %.

# Machine Learning methods for real estate price predictions, however, most of the
# do not make use of deep learning, and even if they do, they do not make use of
# the actual floor plan, but mostly rely on structured explanatory variables for
# the their models. Some recent examples include,  and
# [cite:@sangha21_proper_norweg] , for the German housing market and
# autocite:sangha21_proper_norweg for the Norwegian housing market. In order to
# compare our results to other results, we also reconstruct some models often used
# in the literature as baseline models.

#+end_abstract

* Introduction [This is still pretty poorly written]

The problem of real estate value estimation comes up in multiple forms throughout economics and society.
For example real estate appraisers determine the value of a property used for the determination of the network of individuals.
This kind of real estate appraisal is often done on an individual basis, with nationally certified appraises estimating the value of a house.
In a slightly different context, and on a larger scale, the potential rent that a property would get on the market at a given point, has to be estimated or imputed for use in the determination of economic variables as for example the GDP.
In economics, real estate prices are often considered to be hedonic prices, meaning that the final price of a good is determined by the parts that make it up, and hedonic methods are often
the de factor methods for real estate price estimation, including the above two use cases.
For example, in the U.S. state Utah real estate appraisal has to be done via hedonic estimation.
For the second use-case, the methods used for estimation are almost exclusively done via hedonic methods. [cite:@weko_55303_1]

For these reasons, much research on real estate pricing has been conducted and most of this research makes use of purely tabular data.
However, non-tabular data, like the actual floorplans of the real-estates, contains valuable information not easily expressed in a more tractable format, we will examples for some of this
information in section.
The underlying hypothesis of this research paper is that the floorplans, which are intrinsic to any given property, expose some of these properties, can be leveraged with Deep Learning, and advance our insight into the price structure of real estate.

We collected tabular data and the floor plans of over 100,000 rental apartments in the Tokyo Metropolitan area from a publicly available website. Then we used the Neural Network architecture
suggested in [cite:@he15:deep_resid_learn_image_recog], to predict the rent of each apartment given only its floorplan, and lastly we used these predictions together with a more traditional hedonic regression model to ascertain the effect that the Neural Network has.

We focused on rental apartments in Tokyo in particular because, as outlined in [cite:@weko_55303_1] and [cite:@akiyama_yuki2019320204], estimating real estate prices throughout different markets becomes more challenging.
In @akiyama they have to rely on the K-Means++ algorithm to create clusters for different real estate markets, which they then use in their subsequent analysis.
As this is mostly an exploratory study, we decided that by focusing on "roughly" a single market, we can sidestep this complication and focus on the topic at hand.

** Findings
We found that


* Background

** Current real estate models
Most real estate models are based on easily observable features of each kind of
rental apartment, such as the zip-code, the closest station, or the number of
rooms in each apartment. It

** We want to consider the room layout
However the actual layout of each apartment could also be considered to a very
important aspect of each apartment, however, since it is hard to describe the
apartment layout precisely in words, in other words, hard to categorize, most
analyses of house prices thus ignore the problem of precisely incorporating
information about the apartment layout.
** 2 main questions
In this paper we will try to answer 2
main questions.

 1. Does having information about the room layout, help with the prediction for
    any model? To test this we will try to predict the price of the apartment
    solely with the CNN
 2. If there is an effect of considering the room layout, how do we build the
    model that is most efficient for prediction.
    1. For example we could use the output of our model together with a linear
       model for the categories, like in 1.
    2. We could make it kinda ensemble like, using the output of multiple
       models, and using another linear model to combine them ( Or other
       ensemble methods )
    3. We could use a multimodal approach, where we output latent states with
       each of our submodels and use a "head" model to fuse our latent states
       as described in:

* Data
The dataset used is a mix of tabular and image data of rental real estate
properties from the Tokyo Metropolitan area, listed on a public website. The
data was collected for the purpose of writing this paper.

For each listing, we have the monthly rent of the apartment, the image of the floorplan of the apartment, 5 continuous and 3 categorical variables. The details for the tabular variables are
described in [[var_explanation]].

#+CAPTION: An explanation of the variables that were collected and used in this study.
#+label: var_explanation
#+Name: var_explanation
|-----------------+----------------------------------------------------------------------------------------------------|
| Variable        | Explanation                                                                                        |
|-----------------+----------------------------------------------------------------------------------------------------|
| station         | Name of the closest public transport station, (by travelling period)                               |
| method          | Description of the layout type of the apartment of the type of apartment (for example 1K, 1LDK...) |
| apt_style       | Number of floors of the building of this listing                                                   |
|-----------------+----------------------------------------------------------------------------------------------------|
| apt_floor       | The floor the property is on property                                                              |
| apt_size        | Size of property in $m^2$                                                                          |
| time_to_station | No. minutes of taking "method" to the next station                                                 |
| b_age           | No. of years ago the property was built                                                            |
| b_no_floors     | No of floors of the building                                                                       |
| apt_admin_price | Amount of monthly administration fee                                                               |
|-----------------+----------------------------------------------------------------------------------------------------|
| apt_rent        | Rent per month of the listing. In units of 10000 Yen                                               |
|-----------------+----------------------------------------------------------------------------------------------------|

The data collected is observational only, and not representative of the Tokyo
realestate market as a whole.

** Variables Used
** Summary stats
** Some more data explanation with some plots

* Models, Methods, Architecture
In this section we will explain the two parts essential parts of the method used in this paper.
First, we will explain how we built the neural network that is responsible for rent estimation
based solely on the floor plan, and after that we will discuss the hedonic pricing method used
for confirmation.
** ResNet
For the construction of the Neural Network we relied on the software libraries,
~fastai~ [cite:@howard20_fastai] and ~pytorch~ . We
build on the ~resnet50~ implementation by ~pytorch~ of the model outlined in
. Furthermore, we initialized the model's
weights to weights that were pretrained on Imagenet.

The training weights were intialized to the values of the pretrained ~resnet50~ model
on pytorch.
 - However we replaced the latter layers by some layers that we added on our own.
   - Neural Net at the end for the regression
   - Sigmoid Layer to scale
   -

The Resnet model and learning rate were optimized with adam, and the learning rate schedule was chosen as suggested in [cite:@smith17_cyclic_learn_rates_train_neural_networ].

*** Preprocessing of the images
We perfomed the same three steps of preprocessing for all of the floorplan images. While the first step of normalization is quickly explained, rotation and especially resizing
carry with them some complications, so we will discuss some trade offs we had to make in this section.

1. Normalize
2. Rotate
3. Resizing

Normalization of the pictures was done using the means and standard deviations for the of the pretrained model.
All of the input images of the original model were normalized, and thus all of the weights
are calibrated to expect normalized inputs. Should inputs not be normalized with the same values, the model's predictions might behave unexpectedly.

**** Rotation

The rotational step is mostly implemented as a form of data augmentation. For each
image there is a 25% chance, to be rotated either 90, 180, 270, or 0(360) degrees.

This is done because while the floor plans usually do have characters or writing in them
they do not have an intrinsic "direction", and many floor plans have a compass rose printed on them.
By randomly rotating floorplans between the different examples, we aim to increase the amount of examples of room layouts the model gets to see, and enhance the model's generality. ..?

**** Resizing

In order to efficiently process images on the GPU, all images have to have the
same dimensions. However the images in the dataset colected had different
dimensions, so the images have to be cropped. We cropped all images to 224x224
pixels. The choice for this size seemed to be a good fit since most images in
our dataset are between 200-400 pixels in both, height and length. Other than
for this reason, however, the size was picked mostly out of convention. The
images were cropped lazily before feeding them into our model, so we were able
to try different approaches to cropping the image. We found that the third
approach described below worked best for our dataset.

1. Cropping out just the middle part of the image and padding with black if the
   images height or size was smaller than the 224 pixels. One drawback with this
   method is that if we were to crop out an important part of the image, there
   would be no information for the model to refer to. Furthermore, the padded parts
   are wasted computations.
2. Distorting the image so that fits into the 224 pixels by "squashing" it into
   the 224x224 pixels. With this approach it is possible to retain all parts of the
   image, so that no important parts are left out, however, when resizing like this,
   the amount of "squishing" done for each image varies, so the model has to learn
   to deal with different amounts of distortion.
3. Cropping out not the middle part of the image, but a random part of the
   image. This problem entails with it the same problem as approach 1, however
   by cropping a random part, rather than just the center, we have more possibility of training the neural network, since even if we use the same image twice, there is a
   high probability that the images are cropped differently. This is a technique that
   is often used as a means of data augmentation as well.
We had the best results for training the "Vision" part of our Neural network,
when using the third approach, and results reported in this paper were done with
"Random Crop" strategy.

[[floorplan_transforms]] shows how different techniques influence the different cropping methods.
#+HEIGHT: 500
#+CAPTION: This figure showcases the properties of each kind of resize as outlined in the paper. The first row shows some 9 floor plans where the data has been Resized with the "squish" method. The second row shows the same picture with the crop-and-pad method, and the third row shows the first apartment of the other two rows, randomly cropped to different sizes. The black parts of the second and third rows are the padded parts. For clarity, this image does now show any of the rotations performed.
#+name: floorplan_transforms
#+label: floorplan_transforms
[[file:./assets/resizes.jpg]]

** Hedonic Price estimation
The hedonic price estimation is done via a Multiple Linear regression model making use of all the variables collected in the following way.

\[  y = \alpha + \beta X + \lambda minutes * way + e \]

We used the below model, X is the design matrix of all variables listed in
[[var_explanation]], except for the categorical variable "way", and the continuous
variable "minutes", which were modelled to be an interaction between the
variables, because we would expect the number of minutes by car to the next
station to affect the price of the rent differently than number of minutes by
foot.

* Results
** Quantitative
*** Results for our model
On its own the model has ....

#+INCLUDE: "table2.tex" export latex

*** Results for vision model put into Linear Regression ( Is the prediction column statistically significant? )
*** Results of baseline models
** Qualitative
Furthermore, we did some other Qualitative investigations into whether the model seems to be able
to incorporate the information obtained from the floorplans.
One example, that the model would definitely have to be able to solve with a reasonable efficiency is that
of the classification of the rooms' layout based on its floorplan. This is because a rooms' layout description
should be fully described by its floorplan, as supposed to the rent of the apartment, which is influenced
by other factors outside of the floorplan as well.

*** Lowest predictions
*** Highest predictions
[[./assets/random_table.png]]
*** biggest discrepancy

* Discussion

Some of our results are hard to interpret, e.g. multimodal learning is worse than the ensembling method. (probably) Why?
** Problems of very high dimensionality due to many many categories in the categorical variables.
* Conclusion
** Conclude whether using these models might make sense or not
Some of our
** Further possible investigations
Some possible talking points:
 - If the results are good, would looking at a bigger market be interesting
 - If we had a more representative sample, could we use some of the results to make some
   interesting conclusions
 - It would be interesting to analyze the outputs using methods as described in for example with shap or eli5, to see
   why it doesn't work if it doesn't or what it focuses on for certain predictions, if it does.
 - How does everything look for the multimodal approach, does it make sense or not?
And ways to improve the model

** Cite Fastai

\printbibliography
