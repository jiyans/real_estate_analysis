#+title: [COMPLETE PAPER] Using Deep Learning to Assist Modelling of Tokyo Real Estate Prices
#+SUBTITLE: An evaluation of an incorporation of deep learning methods for prediction real estate prices
#+AUTHOR: Jiyan Jonas Schneider
#+DATE: 2021-12-20
#+LATEX_HEADER: \usepackage{xeCJK}
#+BIBLIOGRAPHY: /Users/jiyanschneider/Dropbox/Documents/lib/bibliography/bibliography.bib
#+LATEX_HEADER: \setCJKmainfont{HiraginoSans-W3}
#+LATEX_HEADER: \usepackage[backend=biber, style=apa]{biblatex}
#+LATEX_HEADER: \setmainfont{EBGaramond-Regular}
#+LATEX_HEADER: \usepackage[a4paper,left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
#+LATEX_HEADER: \usepackage{graphics}
#+LATEX_HEADER: \usepackage{rotating}
#+LATEX_HEADER: \usepackage[doublespacing]{setspace}
#+LATEX_CLASS_OPTIONS: [11pt,titlepage]
#+OPTIONS: toc:2 H:4


#+begin_abstract
This study strives to examine whether consideration of floorplan images of
real-estate apartments could be effective for improving rental price
predictions. We use a modern computer vision technique to predict the
rental price of apartments using the floorplan of the apartment exclusively.
Afterward, we use these predictions combined with a more traditional
hedonic pricing method to see whether its predictions improved. We found that
by including the predictions, we were able to increase the accuracy of the
predictions from an \( R^{2} \) of 0.915 to an \( R^{2} \) of 0.945. This
suggests that floorplans contain considerable information about rent
prices, not captured in the other explanatory variables used. Further
investigation, including more explanatory variables about the apartment itself,
could be used in future research to examine the price structure of real
estate further and better understand consumer behavior.
#+end_abstract

* Introduction
The problem of real estate value estimation comes up in multiple forms
throughout economics and society, with many kinds of agents being interested in
predicting housing and real estate prices. Appraisers and tax assessors conduct
real-estate appraisals, investors conduct fundamental value analyses
[cite:@krainer2004], and it is reasonable to assume that other agents such as
banks have methods of assessing the price of a property as well, for their
mortgage lending operations. The real estate appraisal done by appraisers and
tax assessors is often done individually. This kind of appraisal is usually done
on a by-house basis, with nationally certified appraisers estimating the value of a
house.

However, in most other contexts that are on a larger scale, real-estate prices
have to be estimated with more quantitative methods. For example, the potential
rent that an estate would receive on the market at a given time has to be
estimated or imputed to determine certain economic variables such as the GDP. In
economics, real estate prices are often considered hedonic prices, meaning that
the final price of a good is determined by the parts that make it up, and
hedonic methods are often the de facto methods for real estate price estimation.

These methods, however, do not readily incorporate information about the real-estate
properties at the apartment level, as this kind of information is hard to obtain and formulate.
Most previous research uses tabular data for price estimation, and detailed information
on, for example, apartment-level is not available. Such features might include the size
of the kitchen or the bathtub. However, such information would be expected to affect real-estate prices as well.

The underlying hypothesis of this research paper is that the floorplans contain
some characteristics that are valuable to consumers, neglected with traditional
methods, and can be leveraged with Deep Learning. In the long run, we hope to
advance insight into the price structure of real estate and consumer behavior
surrounding real estate using such methods.

To test our hypothesis, we first collected tabular data and floorplans of over
140,000 rental apartments in the Tokyo Metropolitan area from a publicly
available website. Then we devised some minor changes to a widely used neural
network architecture and trained a NN (NN hereafter) to predict the
rent of an apartment given only its floorplan. Finally, we combined the NN's
prediction with a traditional hedonic regression model and analyzed the effects
of including the NN's predictions.

Doing this, we find that the NN can explain a considerable proportion of the
rent price of many real-estate properties and considerably improve the
explanatory power of the hedonic regression model. Including the NN
does not overly take away explanatory power from variables that would be
considered important in a more traditional model or affect the other variables
in other unexpected ways, showing that the NN captures at least some
previously unconsidered factors.

The structure of the remaining paper is as follows: First, we will introduce the
reader to some background about the topic of real estate prices. After that, in the
[[Methodology]] section, we describe the dataset used, explain the Architecture and
training process of the NN, and the linear regression base models we
used. Finally, we will present our results, discussing the
implications and limitations of this study and our methods.

* Background
** Machine Learning methods in real estate
Previous research on using Machine learning methods for real estate data
has been conducted. For example, textcite:hattori2019rent uses Principal
Component Analysis extracts a feature vector from floorplan data and
subsequently uses this vector in various models. However, they report that while
there are slight improvements in prediction, it is challenging to find merits
for using their approach because these improvements are too small.

Likewise, textcite:zeng2019deep use computer vision techniques on floorplan
images. However, they are looking to find out functionalities of different
elements within the floorplan rather than using it for rent prediction. The earliest example of the use of NNs for rent price prediction the author
could find was text cite:limsombunchai2004, in which they already argue for the
use of NNs in real-estate price predictions over hedonic methods in
spite of the hardships of interpreting NNs, simply due to their
increased predictive powers.

Real-estate prices must be estimated in various situations, with varying
amounts of granularity in data and differing objectives. They are often
criticized from various perspectives. For example, mortgage lenders might be
willing to obtain more data to get more accurate predictions
than governments, who want to estimate rents home-owners would receive for
properties on the market. These estimates would then be used to calculate economic variables such as the gross domestic product; or
used in other government statistics. Due to these diverse uses, different kinds of
criticisms are offered. For example, textcite:weko_55303_1 criticizes imputation
methods the Japanese government is using for being too simplistic. They argue
that the locations of properties should be considered on a finer level and that
city planning and housing quality should be considered as well.

Another attempt to use computer vision for estimation was made in
textcite:poursaeed2018. They use pictures of exteriors and interiors of
properties to improve price predictions. They had success with their approach
and improved on the base model used in their study. Their research differs from ours
in that they are not using the actual floorplans of the estates. Their
approach certainly seems valuable, especially for agents that have access to
pictures of the interiors of the apartments. However, floorplans contain
characteristics that are intrinsic to the real estate property
they are describing. While pictures of interiors carry with them much
subjectivity. When analyzing photographs, the resolution, lighting, angles, and
interior furniture would all be expected to affect the evaluation of a property.
Floorplans, however, especially when prepared professionally, include
information inherent to a specific estate, for example, the sizes of the
different rooms and their relative location to each other.

** Price structure of real estate
Moving onto the price structure of real-estate, textcite:krainer2004 note, using
time-series data, that house price and rent price, while related, are not the
same, and identify the change in the price-rent ratio between the year from 1982
to 2002. Determining the price structure of real estate, in general, is more
complicated than doing a regular regression, as noted in
textcite:rosen_1974_hedonic,nelson1978. Most studies in the
literature, starting from textcite:nelson1978, assume that factors for real-estate
prices can primarily be divided into two categories, the physical and environmental characteristics
of the property. This paper will refer
to what textcite:nelson1978 called "physical" characteristics as structural
"characteristics". Examples of such characteristics might be
an apartment's size, the number of rooms, or layout type. On the other hand, the
environmental factors are not directly included in the apartment but are related
to the environment of the properties' location. Examples could be the distance
to the closest schools, the city center, or the nearest train station. The
structural and environmental factors might change depending on the market as
different markets tend to have their peculiarities. For example, in big Japanese
cities, people spend a higher proportion of their income on rent and tend to put
a higher relative value on property location than in other places.
[cite:@salarymen1999]

The importance of finer subdivision of markets is a topic that often comes up
in the Discussion about real-estate price estimation and is dealt with in
different ways. An example of an advanced method of subdivision of markets
is textcite:akiyama_yuki2019320204. They use a vast dataset containing
information on buildings throughout all of Japan. They rely on the K-Means++
algorithm to create clusters and use the Gap statistic to determine the optimal
Amount of clusters to use. Their subsequent analysis uses these clusters
to analyze their data on a by-market basis.

Lastly, textcite:choi2003, a study about the causes of residential satisfaction.
In the study, they conducted a survey, and one of their findings is that the above
two categories of "structural" and "environmental" can be further subdivided. This study's unique quality is that it considers real estate prices not only
econometrically but also tries to understand their users' psychology in more
detail. This is relevant for our paper because it might give insight into
the price structure of real estate. Their survey asks its participants
to list their satisfaction levels about several items in their current
apartment. In total, they list 35 items. These items are initially subdivided
into the aforementioned categories: "Structural" and "Environmental".
While conducting their analysis, they obtain results of how much each of the 35
items influence overall residential satisfaction. Furthermore, they establish
that the above two categories could be further subclassified. Roughly, they
subdivide the items in the "Structural" category further into "Fundamentals (基
礎環境)[fn:2]" and "Size and Facilities (広さ・設備)". In table
[[tab:satisfaction_items]], we show the items that the survey participants were
asked about. These items are items from the "Structural" category and were
further subdivided into the categories shown. Their study also shows that these
two categories have the most impact on residential satisfaction, and in
particular, have a larger impact on satisfaction than the environmental factors.
#+LABEL: tab:satisfaction_items
#+NAME: tab:satisfaction_items
#+ATTR_LATEX: :name tab:satisfaction_items :label tab:satisfaction_items
#+CAPTION: Table describing the groups and items participants were asked to judge their satisfaction by, in textcite:choi2003.
| Fundamental           | Size and Facilities        |
|-----------------------+----------------------------|
| Lighting              | Overall Size               |
| Ventilation           | Floorplan                  |
| Condensation (or dew) | number of rooms            |
| Insulation            | Interior design and Finish |
| Noise                 | Storage space              |
| Outward appearance    | Kitchen space              |
|                       | Kitchen facilities         |
|                       | Washing room space         |
|                       | Washing room facilities    |
|                       | Bathtub size               |
A similar conclusion is shared by textcite:akiyama_yuki2019320204, who find
that structural characteristics are generally preferred over environmental
features, even in Tokyo, where environmental features are regularly valued more
highly than in other parts of Japan. They also conclude that
the security level of the environment an apartment is located in is mostly
unconsidered by most people when choosing where to live. Here, "security" refers
to an estimated risk of destruction due to earthquakes or other natural
disasters.

While structural features seem to be more impactful, they are also harder to
obtain. Although apartment size is a standard characteristic available in most
datasets, more specific features like washing room or kitchen space are
harder to acquire. However, many of the structural features are available in
floorplan information. For example, while about residential satisfaction and not
explicitly about rent prices, we expect that floorplans contain information
about all features in the "Size and Facilities" section. Furthermore, many
floorplans even contain information about balconies, windows, the cardinal
direction of the apartment, and some even include the placement of air
conditioning machines. This information could help determine some of
the items listed in "Fundamentals".

Lastly, floorplans also expose information about more complicated notions that
were not posed in the survey of textcite:choi2003, possibly because they are too
complicated. Examples of this could be the proportions of rooms relative to each
other, the location of shower and toilet (for example, whether they are in the
same room), or how much cooking space is available in the kitchen.

* Methodology
** Data
The dataset used is a mix of tabular and image data of rental real estate
properties listed on a public website from the Tokyo Metropolitan area. The
data was collected to write this paper.

We focused on rental apartments in Tokyo in particular because, as outlined in
textcite:moriizumi1986;weko_55303_1 and textcite:akiyama_yuki2019320204,
estimating real estate prices throughout different markets are more challenging and causes complications as described earlier. This paper is primarily an exploratory study, so
we decided that by focusing on "roughly" a single market, we can sidestep the problem of considering multiple markets and focus on the viability of these computer vision methods first.

For each listing, we have the monthly rent of the apartment, the image of the
floorplan of the apartment, 6 continuous and 3 categorical variables. The
details for the tabular variables are described in Table [[tab:var_explanation]].
#+LABEL: tab:var_explanation
#+NAME: tab:var_explanation
#+CAPTION: Explanation of the variables collected and used in this study.
#+ATTR_LATEX: :name tab:var_explanation :label tab:var_explanation
|------------------------+-----------------------------------------------------------------|
| Variable               | Explanation                                                     |
|------------------------+-----------------------------------------------------------------|
|------------------------+-----------------------------------------------------------------|
| Apt. Floor             | The floor the property is on property                           |
| Size in \(m^{2}\)      | Size of property in $m^2$                                       |
| Time to station        | No. minutes of taking "method" to the next station              |
| Age bldg.              | No. of years ago the property was built                         |
| No Floors bldg         | No of floors of the building                                    |
| Admin fee  \(10,000¥\) | Amount of monthly administration fee                            |
|------------------------+-----------------------------------------------------------------|
| Station                | Name of the closest public transport station                    |
| Method                 | How "Time to station" is measured (foot, bus, or car)           |
| Style                  | Description of the layout type of the apartment (1K, 1LDK,... ) |
|------------------------+-----------------------------------------------------------------|
| apt_rent               | Rent per month of the listing. In units of 10000 Yen            |
|------------------------+-----------------------------------------------------------------|
The data collected is observational only and not representative of the Tokyo
real estate market as a whole.

In figure [[fig:hists]] we can see the distribution of values our variables take on.
We include further summary statistics in the appendix (Table [[tab:app:summ_cont]]
and [[tab:app:summ_cat]]). We see that the rent price, "apt_rent", has a very
long, thin tail to the right. After taking the logarithm, the values move
closer to 0 and take on a more symmetric shape. As for the distribution of
building age, we can see that the number of buildings declines with age. In the
number of floors of the buildings, we see a spike at two floors, then two
sudden declines, at 5 and 15 floors respectively. The reason for these drops
most likely lies in a change of building regulation at certain heights. In the
"Time to station" variable, we also observe some irregularity around the 5, 8,
and 10-minute marks, there being sudden declines at each of the values. While
most of these times were given in minutes by foot, some also were given in
minutes by Bus (1041 cases) / and minutes by car (24 cases), which can be seen
in the "Methods" plot and is the reason the "methods" variable was included.

#+NAME: fig:hists
#+LABEL: fig:hists
#+ATTR_LATEX: :name fig:hists :label fig:hists
#+CAPTION: This figure depicts the value frequencies of the variables used in our paper. Note that for the "Styles" variable, the x and y axes have been flipped for the readability of the labels.
[[./assets/varhists.png]]

"Styles" shows the distribution of the layouts classifications of the rooms. 1R
means "One room".[fn:1] The original dataset included layouts of apartments
until "11LDK", leading to a high cardinality. We included all layouts with more
than five bedrooms under the category "5+", which seems to be a comparatively
small group nonetheless. We can see that other than 1K and 1R, the "LDK" type
rooms seem to be the most popular layouts.

Figure [[fig:corrplot]] shows a correlation plot of all of the variables. Many of
the explanatory variables are quite highly correlated. This high correlation means
that an interpretation of coefficients in a linear regression model would be less
reliable. In this paper, however, we will not make any strong interpretations
based on our coefficients.
#+NAME: fig:corrplot
#+LABEL: fig:corrplot
#+ATTR_LATEX: :label fig:corrplot :name fig:corrplot :width 8cm
#+CAPTION: Heatmap showing the correlations of the variables displayed in [[tab:regression]].
[[./assets/corrplot.png]]

** Neural Network architecture
This subsection will explain the architecture of the NN we used and the preprocessing and augmentation steps we performed.

For the construction of the NN, we relied on the software libraries
~fastai~ [cite:@howard20_fastai], ~pytorch~ and ~torchvision~. (Pytorch and torchvision both by the PyTorch team [cite:@NEURIPS2019_9015]). We built on the ~resnet50~ implementation by
textcite:NEURIPS2019_9015 of the model outlined in
textcite:he15:deep_resid_learn_image_recog. We initialized the model's weights
to the pre-trained weights available in ~torchvision~. These weights are trained
using the "ImageNet" [cite:@imagenet2009] dataset.

We replaced the last layers with a custom adaptation to better fit the
regression task at hand. We used the same base model up until the first fully
connected layer, which we replaced with another untrained fully connected layer.
Initially, we used a fully connected layer with a single output as the final
layer. However, we found that sometimes the model would make unreasonably high
predictions, which complicated the model training by abnormally increasing the
loss, resulting in "exploding gradients". Thus, we decided to add
another layer to scale the NNs output between a predetermined range.
In particular, we scaled the last layer's outputs with a sigmoid function. By
scaling the outputs of The NN, we could prevent these problems at
the expense of introducing one hyperparameter, the y-range. To decide on the
y-range for our NN, we used the log-transformed target variable's
greatest lower and least upper integer bounds. Since the extreme values were
0.095, and 5.521, so we chose 0 and 6 as our bounds. This layer scales the
output vector from the Network elementwise according to the following rule.
\(s(x) = \sigma(x) (h - l) + l \), where \( \sigma(x) = \frac{1}{1+e^{-x}} \), \( l \)
is the lower bound, and \( h \) is the upper bound. The outputs of this
function are then used to calculate the loss, ensuring that initial predictions
of the network are never unreasonably high, ultimately resulting in easier
training and convergence.

We used the mean squared error as a loss function, and before training the whole
model, we "froze" the base model and trained our custom head only. After
initial rounds of training the head only, we "unfroze" the pre-trained weights
and trained the whole NN. The Resnet model was optimized with Adam
[cite:@kingma2017adam], and the learning rate schedule an initial learning rate
was chosen as suggested in
textcite:smith17_cyclic_learn_rates_train_neural_networ.

*** Preprocessing of images
We performed the three steps of preprocessing for all of the floorplan images.
1. Normalization
2. Rotation
3. Resizing
**** Normalization
We used the means and standard deviations of the pre-trained model to normalize
all input images. The original model was normalized with those weights, and thus
all of the weights are calibrated to expect normalized inputs. Should our inputs
not be normalized with the same values, the model's predictions might behave
unexpectedly.

**** Rotation
The rotational step is implemented as a form of data augmentation. For each
image, there is a 25% chance to be rotated either 90, 180, 270, or 0 (360)
degrees. This is done because some floorplans usually have characters
or writing in them, but the images themselves do not have an intrinsic direction.
With only its orientation changed, the same floorplan should functionally still
be considered the same floorplan. Furthermore, many floorplans have compass
roses on them to find out the orientation of the rooms. Note that mirroring the
images would change the compass' orientation. Thus we can easily see that
mirrored floorplans are not functionally the same, which is why we avoid
mirroring our images.

**** Resizing
All images have to have the same size for efficient processing of the images.
However, the images in the dataset collected have different sizes, so we had
to choose how to prepare the images. We decided to choose 224x224 pixels for our
images. This size is a conventional choice, which we did not find any
problems with. Most images in the dataset are between 200 and 400 pixels in
height and length, and we did not see a reason to adjust the conventional size. Images were cropped
lazily before feeding them into our model, so we were able to try different
approaches to resizing the image. Each approach has advantages and disadvantages, which we will outline below. We tried the following three approaches.

/Distorting/ the image by "squashing and squeezing" it to fit into the
224x224 pixels. This approach makes it possible to retain information from
all image regions. However, when resizing this way, the Amount of distortion
for each image varies based on the original size, and the model has to process
different degrees of distortions and the distorted proportions these entail.

/Cropping out/ the center part of the image and padding with black if the
image's height or size is smaller than 224 pixels. One drawback of this method
is that if we excluded an essential part of the image, there would be no
information for the model to refer to. Furthermore, since the padded values are
all 0's, they result in wasted computation and more inefficient training.

/Randomly cropping out/ a part of the image with the desired size. This method
has the same problem as the second approach, however by cropping out a random
part, rather than just the center, we can input the NN a
wider variety of images, since even if we use the same image twice, there is a
high probability that the images are cropped differently. Furthermore, at evaluation
time we can crop the same image a few times, predict the different crops and
then average their predictions to obtain a prediction that considers more
area of each floorplan.

Figure [[fig:floorplan_transforms]] (Figure [[fig:app:resizes]] in the appendix shows a larger larger version) shows how different techniques influence the
different cropping methods and let us observe some of the problems outlined.
The first row shows the distorted images. All images with the label 7.7 look
highly alike and are actually from the same building. However, while the first
and eighth picture images look the same, the ninth does not because the original
image contains more white on either side. Even though all three images depict rooms
with almost the same layout, one room looks quite
different from the other two after the distortion. In the second row, we can see that the compass rose of the pictures
labeled 8.0 and 8.57 is cropped out, while it is included in their squished
versions. The last row depicts some images obtained using the random crop
method for the floorplan in the first column of the above two rows. While we get
most of the details, with the correct proportions, due to the random nature of
our cropping, we do not have any image containing the apartment's balcony. The
third approach yielded the best results in some preliminary experiments. Thus
all results in this paper are reported using a model trained on predictions by
using the "Random Crop" strategy.
#+name: fig:floorplan_transforms
#+label: fig:floorplan_transforms
#+CAPTION: This figure showcases the properties of each resizing method. The first and second rows compare nine floorplans. The third shows different crops of the leftmost floorplan.
#+ATTR_LATEX: :name fig:floorplan_transforms :label fig:floorplan_transforms :width 15cm
[[file:./assets/resizes.jpg]]

** Hedonic Price estimation
The hedonic price estimation was performed via a multiple linear regression
model using all variables collected and outlined in [[tab:var_explanation]]. We
log-transform the target variable of apartment rent. While preliminary tests of
the multiple regression model only showed a slight improvement in $R^{2}$, and
the coefficients, the NN's predictions improved significantly.
Furthermore, many of the research papers cited in this paper use log-transformed
rents as well, so we will side with convention. We created dummy matrices for
each of our categorical variables, ending up with 724 columns, including the
intercept and continuous columns. The "station" variable's cardinality of 684
Moreover, the "style" variable's cardinality of 31 caused this significant increase in
dimension. Furthermore, we added a squared term for the "Time to station."
variable to the design matrix. We estimated three different models, one using
all variables, without the rent prediction of the NN, one using all
variables with the rent prediction of the NN, using only the neural
network and an intercept.
* Results
In this section, we will first describe the results of our analysis. Table
[[tab:regression]] shows the results for three models described in [[Hedonic Price
estimation]]. The first column shows the estimated coefficients and standard
errors, without the predictions obtained from the NN (hereafter
referred to as NN Factor), the second shows the estimated coefficients with the
predictions and the last shows the values for a Linear Regression model with
intercept and the predictions of the NN only. We included all
categorical variables in both of the first two regressions, but did not include
their coefficients in the table due to their high cardinality.
#+NAME: tab:regression
#+LABEL: tab:regression
#+ATTR_LATEX: :label tab:regression :name tab:regression
#+INCLUDE: "assets/table2.tex" export latex
We observe a considerable increase in the model's predictive power using
the NN's predictions over the one that does not include the NN's predictions.
The \( R^{2} \) value moves from 0.915, to 0.945, and the Residual Std. Error is
reduced from 0.127 to 0.101, a reduction in error of \( \approx 20\% \). The
signs of the coefficients in the models are as one would expect them to be and
do not change with the inclusion of the NN prediction. However, the
magnitude of the coefficients moved toward 0 in every case. Furthermore, the
previously non-significant factor of "Admin fee" became significant after the
inclusion of the new feature. A similar pattern holds for the variables included
in the regression but not in this table. Most of these coefficients moved toward
0, neither changing sign nor significance.

* Discussion
Our discussion section consists of two parts. The first part discusses the
predictions of the models, the shortcomings of the models, and some potential
remedies. The second part discusses the overall results of our papers in a general
sense.
** Discussion and critique of the Neural Network
We will look at our models' predictions more
qualitatively in this first part. In figure [[fig:residual_plots]] we plotted the predictions against
their actual values to try and see whether there are any patterns of
mispredictions in our models. We can see that all of our models seem to
overpredict the most expensive properties. In [[fig:residual_plots]] we drew a
dotted line at \(x=log(100) \approx 4.6\). After this point, all models seem to
overpredict some rent prices greatly. While the linear models have few but very
high over-predictions, the NN's residuals are smaller but have
shifted systematically above the identity line. The reason for the relatively
low residuals of the NN is that we scaled its predictions with the
sigmoid layer discussed in [[Methodology]]. The NN
also underpredicts many low rent properties, possibly because, in some of these
cases, the properties have a good location, due to which they have their higher
than predicted prices. Overall, we can see that the second model's predictions
are wound around the identity more tightly, and its predictions seem to have
been improved.
#+LABEL: fig:residual_plots
#+NAME: fig:residual_plots
#+ATTR_LATEX: :height 4cm :label fig:residual_plots :name fig:residual_plots
#+CAPTION: This image shows the residuals of the predictions of the two models and the NN. The x and y axes for all of the plots are the same. The dotted blue line is drawn at \( x = log(100) \approx 4.6 \).
[[./assets/residuals.png]]

Next, we will look at some predictions of the NN and their
floorplans, first looking at some randomly chosen predictions to see what a standard floorplan and its predictions might look like. Afterward, we
will look at the highest and lowest predictions that the model made. A sample of
randomly extracted images is shown in figure [[fig:random_examples]]. The Neural
network is not radically off with any prediction in this example, with the
highest difference in prediction and price being 17000¥. Seen in relative terms,
the model overpredicts the apartment's value by about 25%. However, it is
hard to tell exactly why the NN made the predictions it did, and
some of the more extreme predictions show more easily discernible patterns.
While some of these extreme predictions come about simply due to problems with
the dataset. They also provide insight into how the NN is
making its predictions. That is why instead of removing these cases and retraining
the model right away, we decided to explore them in this section. As these
images are small, we will provide larger versions in the appendix.

#+LABEL: fig:random_examples
#+NAME: fig:random_examples
#+ATTR_LATEX: :height 4cm :label fig:random_examples :name fig:random_examples
#+CAPTION: This image shows the NN's predictions and ground truths for a randomly extracted sample of the dataset. (in 10,000¥)
[[./assets/random_table.png]]

Figures [[fig:negtop]] and [[fig:postop]] exhibit images that the NN's predictions were the lowest and highest for.
#+LABEL: fig:negtop
#+NAME: fig:negtop
#+ATTR_LATEX: :label fig:negtop :name fig:negtop :height 4cm
#+CAPTION: These are the four predictions the model predicted the lowest rent for. (in 10,000¥)
[[./assets/rand_neg_top_100.png]]

The floorplans for figure [[fig:negtop]], the models' lowest predictions, are solely
for dormitory or boarding house-like apartments. The model seems to have picked
up on the repetitive pattern in the floorplan often present in these apartments.
Although the overall size is of the floors is quite spacious, and the floorplan
spans multiple floors. The model's predictions, having seen some amount of these
kinds of plans, seem to predict the price for only a single room. However, as we
see later in figure [[fig:upward]], the model cannot always correctly tell these
kinds of shared-living spaces from big apartments intended for a single
household. Note that the predictions for the middle two floorplans were
different even though the floorplans are the same. This is due to the
preprocessing step where we randomly crop our images. When making the two
predictions for the middle floorplans, the model thus had slightly different
inputs and outputs. This image appears twice in the dataset because two
different rooms in this building were open for rent, which explains the
difference in actual prices. If the prices for each room are close, like in this
case, the model does not have any information about which room to
predict the rent for is not overly detrimental. A potential failure point could happen when several apartments that differ significantly in rent
prices are depicted on the same floorplan.

#+LABEL: fig:postop
#+NAME: fig:postop
#+ATTR_LATEX: :label fig:postop :name fig:postop :height 4cm
#+CAPTION: The floorplans of the four apartments with the highest predicted rents. (in 10,000¥)
[[./assets/rand_top_100.png]]

Figure [[fig:postop]] shows the floorplans with the highest predictions, and the
residuals are much higher relatively as well as absolutely. The model appears to
choose spacious apartments with multiple floors, prominent balconies, and a
non-repetitive layout for its highest predictions. Overall, however, it is
harder to find a definitive pattern in the highest predictions of the model.

Next, to see some more edge cases, we will present the predictions that
changed the most due to the input of the NN factor. First, we will look at the
largest downward changes due to the NN, and afterward, we will look
at the largest upward changes.
#+LABEL: fig:downward
#+NAME: fig:downward
#+ATTR_LATEX: :height 4cm :label fig:downward :name fig:downward
#+CAPTION: This image shows the floorplans of the apartments with the biggest decreases in prediction after considering NN output. (in 10,000¥)
[[./assets/overpreds.png]]

Figure [[fig:downward]] and [[fig:upward]] have a slight change in format. In these
figures, we compare the differences in the predictions of models 1 and 2 for a
given apartment. Furthermore, we also depict the NN's prediction and the
actual rent value. Figure [[fig:downward]] shows the greatest downward shifts due to
consideration of the NN. The property of the middle two floorplans is the same.
It was posted to the website under different names by different agencies
multiple times, even with different prices. It is quite large with an exceptional
location, so the linear models greatly overestimate the rent. The
NN's prediction is comparatively low because we are scaling them into the
correct output range. In the rightmost picture, we can see that the NN's
prediction on its own was far below the property's actual value, while the
linear model's predictions were much closer to it. The reason for this seems to
be the extremely simple floorplan of the property. Upon further investigation,
it turned out that his property is an office space that was mistakenly posted on
the wrong part of the website. In the linear model, the size and location of the
office are the biggest contributors to the prediction.

Finally, we will look at the greatest upward movements after considering the
predictions of the NN.
#+LABEL: fig:upward
#+NAME: fig:upward
#+ATTR_LATEX: :height 4cm :label fig:upward :name fig:upward
#+CAPTION: The floorplans of the apartments with the biggest increases in predicted rent due to the NN's prediction.
[[./assets/underpreds.png]]

The first floorplan in figure [[fig:upward]] was posted for a room available in a
shared flat. For this estate, the prediction of the model without the NN is much more accurate than the other two predictions. This is
because the NN overestimated the rent by a lot. The reason for this
might be that the NN was considering the floorplan as a whole,
while the actual apartment available for rent is only a single room. While in
figure [[fig:negtop]], the NN still predicted these cases more
accurately, here it did not. A similar problem seems to have occurred with the
third floorplan, which displays the apartment's floorplan and a balcony that
everyone living in the building can use. The reasons for the high upward
shift in the second and fourth floorplans are much harder to discern.

** General discussion
In this paper, we found that utilizing the floorplans of rental apartments can
improve the predictive power of linear regression models when not many variables are available.

We suppose that the reason for the effectiveness of the NN is twofold:
1. By using the floorplans, the NN had access to information that
   influences rent and residential satisfaction (as described in
   textcite:choi2003) of a particular real estate. Thus it can find features that
   influence rent that are not available in the tabular dataset, and leverage
   these for its prediction.
2. The increase in explanatory power seems extraordinarily high because of our
   relatively simple dataset. While we had a sizable amount of apartments and
   floorplans, we had much fewer explanatory variables than other studies on
   hedonic pricing. Furthermore, we only had apartments from the Tokyo
   Metropolitan area.
To further expand on the second point, most studies we reviewed made use of many
more explanatory variables, especially about structural features of the
apartments. We presume that by using those, the regression models' predictive
power would increase, and that of the NN would decrease when used in combination
with the new features. However, under circumstances where it might be easier to
obtain floorplans of apartments rather than the tabular data of the categorical
features an approach utilizing computer vision might be worth considering. And
our method could be used by entities who do not have the resources to gather a
dataset of tabular features but could obtain the floorplans.

This study was exploratory only, and further investigation might include how this
method fares with floorplans in different markets. The current dataset only
includes a limited area of rental apartments in and around Tokyo. This, however,
means that the rent prices we encountered did not deviate as much as they would
when considering more markets. We can easily imagine that bigger discrepancies
in rent amount due to location only could disturb our model. The same problem,
less pronounced, is present in the current dataset already because apartments
with mostly the same layout in different locations will have different prices.
One potential remedy for this problem could be training the model on the
residuals of a multiple linear regression controlling for location. Doing this,
it might be possible to reduce some of the effects of location on rent.

Another problem with NNs, in general, is that they are hard to
interpret. This also applies to the current study. We have trouble explaining
why the model is making some of its predictions. textcite:NIPS2017_7062, for
example, provide an approach for general model interpretation, which sometimes
is also applied to computer vision. Analyzing the current model using the
technique outlined there might give us more insight into its internals and observe
its focus when making predictions. This, in turn, might lead to insights into consumer
behavior.

* Conclusion
We used real estate data collected from a publicly available website to train a
residual-based convolutional NN to predict rent prices based
solely on that properties' floorplan. We proposed some tweaks to enhance the
original model to allow for quicker training and convergence in the case of
real-estate prediction. We showed that it is possible to effectively leverage
floorplan image information to improve the prediction of rent prices and that these
predictions can enhance other more traditional models' predictive power. We only
had limited access to detailed information at the apartment level and thus could not test the effectiveness of floorplan image analysis against models
making use of a wider variety of tabular data. We suspect that using floorplan
data could be an option for entities trying to estimate rent prices without the
need for interviewing participants or employing other costly means of gaining
apartment-level information. Our results seem to be in line with existing
literature on the topic of real-estate price composition. Furthermore, we believe
that this paper shows initial evidence that using computer vision for rent
prediction in low data-availability situations can be practical.

\printbibliography

#+LaTeX: \clearpage
#+LaTeX: \appendix

* Appendix
#+LABEL: tab:app:summ_cat
#+NAME: tab:app:summ_cat
#+ATTR_LATEX: :label tab:app:summ_cat :name tab:app:summ_cat
#+CAPTION: Summary statistics for the categorical variables
 | Name    | Unique | Most Frequent           | No. Occurences |
 |---------+--------+-------------------------+----------------|
 | Style   |     31 | 1K                      |          63573 |
 | Station |    684 | 東京メトロ東西線/葛西駅 |           1839 |
 | Method  |      3 | 歩                      |         140329 |

#+LABEL: tab:app:summ_cont
#+NAME: tab:app:summ_cont
#+ATTR_LATEX: :label tab:app:summ_cont :name tab:app:summ_cont
#+CAPTION: Summary statistics for the continuous variables
| Name             |        mean |         std |  min |     25% |    50% |     75% |       max |
|------------------+-------------+-------------+------+---------+--------+---------+-----------|
| Bldg. Age        |   17.701062 |   15.081147 | 0.00 |    4.00 |   15.0 |    30.0 |     99.00 |
| Bldg. No Floors  |    7.300168 |    5.734189 | 1.00 |    3.00 |    6.0 |    10.0 |     60.00 |
| Size \(m^{2}\)   |   30.497512 |   17.251406 | 1.94 |   21.16 |   25.6 |    35.0 |    491.88 |
| Admin Fee        | 6554.481350 | 5220.711042 | 0.00 | 3000.00 | 6000.0 | 10000.0 | 220600.00 |
| Floor            |    4.096327 |    3.610408 | 1.00 |    2.00 |    3.0 |     5.0 |     57.00 |
| Time to station  |    6.034167 |    3.284996 | 1.00 |    4.00 |    5.0 |     8.0 |     40.00 |
| Rent \(10,000¥\) |   11.119562 |    8.232117 | 1.10 |    7.50 |    9.1 |    12.1 |    250.00 |

#+LABEL: fig:app:resizes
#+NAME: fig:app:resizes
#+ATTR_LATEX: :label fig:app:resizes :name fig:app:resizes
#+CAPTION: The floorplans of the apartments with the biggest increases in predicted rent due to the NN input.
#+begin_sidewaysfigure
[[./assets/resizes.jpg]]
#+end_sidewaysfigure

#+LABEL: fig:app:top
#+NAME: fig:app:top
#+ATTR_LATEX: :label fig:app:top :name fig:app:top
#+CAPTION: This figure shows the Minimal and maximal predictions that the NN made.
#+begin_sidewaysfigure
[[./assets/rand_neg_top_100.png]]
[[./assets/rand_top_100.png]]
#+end_sidewaysfigure

#+LABEL: fig:app:impact
#+NAME: fig:app:impact
#+ATTR_LATEX: :label fig:app:impact :name fig:app:impact
#+CAPTION: This figure shows the predictions where an inclusion of the NN's predictions introduced the largest changes.
#+begin_sidewaysfigure
[[./assets/underpreds.png]]
[[./assets/overpreds.png]]
#+end_sidewaysfigure

* Footnotes
[fn:2] They use the word 基礎環境, referring to items such as ventilation, lighting or insulation.
[fn:1] These layout descriptions are interpreted as follows: The Number at the
beginning denotes the number of bedrooms. Following that, come the letters "S",
"L", "D" and "K", which stand for "Service", "Living", "Dining" and "Kitchen."
respectively. Thus a "2LDK" apartment would have two bedrooms, one living room, one
dining room, and one kitchen. Service often stands for a small room without enough
lighting or ventilation to count as a bedroom. These rooms are often utilized
for storage. In figure [[fig:hists]], 1R stands for ワンルーム (1 Room), a layout in
which there either is no Kitchen, or there is no partition between the kitchen
and the bedroom.
