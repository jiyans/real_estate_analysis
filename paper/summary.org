#+title: WIP Using Deep Learning to Assist Modelling of Tokyo Real Estate Prices Summary
#+SUBTITLE: WIP An evaluation of an incorporation of deep learning methods for prediction real estate prices
#+AUTHOR: Jiyan Jonas Schneider
#+EMAIL:     jiyan.schneider@gmail.com
#+DATE:      2021-12-20
#+LATEX_HEADER: \usepackage[backend=biber, style=apa,]{biblatex}
#+LATEX_HEADER: \usepackage{xeCJK}
#+BIBLIOGRAPHY: /Users/jiyanschneider/Dropbox/Documents/lib/bibliography/bibliography.bib
#+LATEX_HEADER: \setCJKmainfont{HiraginoSans-W3}
#+LATEX_HEADER: \setmainfont{EBGaramond-Regular}
#+latex_class_options: [12pt]
#+LATEX_HEADER: \usepackage[a4paper,left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
#+OPTIONS: toc:1

#+begin_abstract
The aim of this paper is to examine the opportunities of application of Computer
vision Deep Learning methods in the realm of housing price prediction. Although
there have been quite a few studies investigating the use of Machine Learning
methods in the area of real estate price prediction, generally they are based on
purely tabular data. The novelty of this paper study lies in the use of Computer
Vision Deep Learning methods to analyze images of floor plans of real estate
listings and use these results to firstly, predict rent prices of properties,
and secondly, for the interpretation of the the way real estate prices come into
existence.
#+end_abstract

* Introduction
Real estate prices have to be estimated for different purposes (real estate
appraisal for evalation of net worth, but also for other national surveys.
Furthermore imputed real estate prices are used in the output of homeowners.
Here the price of the rent is estimated from the price of the house and other
values.)

Because real estate product of a ), and the structure of real estate prices is a
long standing problem. Since [cite:@rosen_1974_hedonic] real estate prices are
often considered to be of hedonic nature, meaning that the price of a certain
property would consists of it attributes associated with the property. The most
tangible example would be, that it is easy to imagine that the rent of an
apartment is a function of its size. While real estate prices are often
considered to be hedonic the factors that make up the prices are still being
researched.

In particular we will try to answer the following 2 main questions

 1. Does having information about the room layout, help with the prediction?
    To do this we will construct a model similar to what is described in [cite:@he15:deep_resid_learn_image_recog]
    to predict monthly rent prices. Afterwards we will use these predictions with other
    predictors regularly used in real estate market models.
 2. If there is an effect of considering the room layout, how do we build the
    model that is most efficient for prediction?
    If there is a positive effect of making use of the image data, how do we best incorporate
    it into our models. In particular, we plan to consider the following 3 kinds of models.
    1. Using the output of the image model for use in a linear model with the rest of the data.
    2. Use the output of the image model together with a Neural Network trained on the tabular information and
    3. We could use a multimodal approach, where we output multiple latent states from each of
       our submodels, fuse these latent, and use a "head" model to fuse our latent states
       as described in.
* Data
The dataset used is a mix of tabular and image data of real estate properties
from the Tokyo Metropolitan area, listed on a public website. The data was
collected for the purpose of writing this paper. The Tokyo Metropolitan area
includes parts of Yokohama, Saitama and Chiba as well. Since our dataset is not
a random sample, our anlaysis can not be considered as an analysis of the Tokyo
realestate market as a whole. Rather it serves as a way for us to outline how to
use of the methods described in this paper might influence more traditional
approaches. However, the results from this paper should not be used for
interpretations on the Tokyo real estate market as a whole.

Two potential sources of bias in the dataset are that we are only considering
properties that were available for rent at the time of data collection, and that
we rely on data from only single website, and are not making use of other
sources. However, these biases are not expected to have an impact on our
results, in this paper we are more concerned with the efficacy of the different
measures, rather than making statements about the Tokyo Real-estate market.

Our dataset consists of 141394 observations. For each of these observations we
had available 6 continuous explanatory variables and and 3 categorical
explanatory variables. Our independent variable is the monthly rent price for
each observation in units of 10000Â¥.

** Variables Used
#+CAPTION: This is a table explaining the variables used in our dataset.
| Variable        | Explanation                                                    |
|-----------------+----------------------------------------------------------------|
| station         | Name of the closest station, measured by time travelled        |
| method          | Description of the type of apartment (for example 1K, 1LDK...) |
| apt_style       | Number of floors of the building of this listing               |
|-----------------+----------------------------------------------------------------|
| apt_floor       | The floor the property is 0.                                   |
| apt_size        | Size of property in $m^2$                                      |
| time_to_station | No. minutes of taking "method" to the next station             |
| b_age           | No. of years ago the property was built                        |
| b_no_floors     | No of floors of the building                                   |
| apt_admin_price | Amount of monthly administration fee                           |
| apt_rent        | Rent per month of the listing. In units of 10000 Yen           |
|-----------------+----------------------------------------------------------------|

** Summary stats
** Some more data explanation with some plots
* Models, Methods, Architecture
The implementation of the models described below were made with the software
library "fastai" introduced in [cite:@howard20_fastai]
** 2 main models
Even though the data consist of Integers only, we will choose the rent price to be

- Cite Resnet paper
- Cite pretraining paper
- Cite Embedding paper
- Cite Learning Rate paper

** Explain the Multimodal model
explained in [MULTIMODALPAPER]
*** Different kinds of multimodals
**** TODO A model that is very similary to the above multimodal model, however the connection
between the two models is not many nodes, but only a single. Pretrained on the regression task
**** TODO A model with 1 CNN and 1 NN
**** TODO A model with 1 CNN and 1 LN I think this should be the model
** Baseline models
Give reasons for why we chose those models ( e.g. other papers used those models )
*** Multiple Linear Regression
*** Random Forest
*** Neural Network
** Practical Adjustments that had to be made
*** Explain how we crop our pictures
In order to efficiently process images on the GPU, all images have to have the
same dimensions. However the images in the dataset colected had different
dimensions, so the images have to be cropped. We cropped all images to 224x224
pixels. The choice for this size seemed to be a good fit since most images in
our dataset are between 200-400 pixels in both, height and length. Other than
for this reason, however, the size was picked mostly out of convention. The
images were cropped lazily before feeding them into our model, so we were able
to try different approaches to cropping the image. We found that the third
approach described below worked best for our dataset.

1. Cropping out just the middle part of the image and padding with black if the
   images height or size was smaller than the 224 pixels. One drawback with this
   method is that if we were to crop out an important part of the image, there
   would be no information for the model to refer to. Furthermore, the padded parts
   are wasted computations.
2. Distorting the image so that fits into the 224 pixels by "squashing" it into
   the 224x224 pixels. With this approach it is possible to retain all parts of the
   image, so that no important parts are left out, however, when resizing like this,
   the amount of "squishing" done for each image varies, so the model has to learn
   to deal with different amounts of distortion.
3. Cropping out not the middle part of the image, but a random part of the
   image. This problem entails with it the same problem as approach 1, however
   by cropping a random part, rather than just the center, we have more possibility of training the neural network, since even if we use the same image twice, there is a
   high probability that the images are cropped differently. This is a technique that
   is often used as a means of data augmentation as well.
We had the best results for training the "Vision" part of our Neural network,
when using the third approach, and results reported in this paper were done with
"Random Crop" strategy.

Figure 1. shows how different techniques influence the different cropping methods.

#+HEIGHT: 500
#+CAPTION: This figure showcases the properties of each kind of resize as outlined in the paper. The first row shows some 9 floor plans where the data has been Resized with the "squish" method. The second row shows the same picture with the crop-and-pad method, and the third row shows the first apartment of the other two rows, randomly cropped to different sizes. The black parts of the second and third rows are the padded parts.
[[file:./assets/resizes.jpg]]

*** Explain what other tricks we used
*** Hyperparameters (Like the learning rate)
*** Explain the embeddings I will use for the Neural Network
For the Neural Network part of the architecture we made use of Categorical Embedding layers
We used the

*** Exactly explain how the model is trained
 - Learning rate adjustment
 - Pretrained resnet 50
 - For the categorical ensemble thing, that first the network is trained,
   then weights are frozen, and that only the new head of the resnet50 is trained at first, for a few epochs,
   and at the end we train both models
 - Same for the output
* Results
** Results of baseline models
ãã¼ã¹ã©ã¤ã³ã®ã¢ãã«ã¯ãããªæãã«ãªã£ã¦ãã¾ãã
æåã®éåå¸°åæä»¥å¤ã¯ç¬ç«å¤æ°ã¯å®¶è³ã®log ã«ãªãã¾ãã

*** Linear Regression
#+CAPTION:  This is is the result of a log linear regression regression performed with only the continuous variables on the log of the apartment rent: N=141394, Corr=0.86 $R^{2}=0.74$ mse=17.65
[[./assets/Linear Regression.jpg]]
#+Caption: This is is the result of a log linear regression regression performed with only the continuous variables on the log of the apartment rent N=141394, Corr=0.86 $R^{2}=0.74$ mse=0.05
[[./assets/Linear Regression log.jpg]]

#+Caption: THis is the result of linear regression all previous variables + apt_style Corr=0.877        N=141394 $R^{2} = 0.77$ mse=0.04
[[./assets/style and log_rent.jpg]]

#+Caption: This ist the result of a linear regression performed on all variables mse=0.03
[[./assets/Station, Style, log rent.jpg]]
*** Random Forest
These are the results for random forest
[[./assets/rf.jpg]]

** Results for vision model
(Is the prediction column statistically significant?)

#+CAPTION:ä¸æ¦å­¦ç¿ãã¦ã¿ãã¨ãã ãã®ãããª0.05ãããã®ç²¾åº¦ã§ããã å¹³åã ããäºæ¸¬ããã°0.17 ã«ãªãã¾ãã®ã§å¹³åããã¯ããã§ããã®ãªã®ãªæ®éã®ç·å½¢åå¸°ã¨åããããªç²¾åº¦ã«ãªã£ã¦ãã¾ãã
[[./assets/viz_learner.jpg]]


** Results Regular NN
#+CAPTION:TabularDataãå¨çµåå±¤ã® input->100->10->10->1ã®ãã¥ã¼ã©ã«ãããã§ä¸æ¦å­¦ç¿ãã¦ã¿ãããããªãã¾ããã
[[./assets/tab_learner.jpg]]


** Results Multimodal

#+CAPTION: ã¾ã éä¸­ã§ãããªããæ§ã¨ã®ãã­ã¸ã§ã¯ãã¨åããããªä½ããä½¿ã£ã¦å­¦ç¿ãã¦ããã¨ããã§ãã éä¸­çµæã¯ãããªæãã§ãã
[[./assets/multimodal.jpg]]

** ã¾ã Vision ã®ã¢ãã«ã®äºæ¸¬ãä½¿ã£ã¦ç·å½¢åå¸°ãè¡ããã¨ã¯ã§ãã¦ãã¾ãã

* Discussion
Some of our results are hard to interpret, e.g. multimodal learning is worse than the ensembling method. (probably) Why?
** Problems of very high dimensionality due to many many categories in the categorical variables.
* Conclusion
** Conclude whether using these models might make sense or not
Some of our
** Further possible investigations
Some possible talking points:
 - If the results are good, would looking at a bigger market be interesting
 - If we had a more representative sample, could we use some of the results to make some
   interesting conclusions
 - It would be interesting to analyze the outputs using methods as described in for example with shap or eli5, to see
   why it doesn't work if it doesn't or what it focuses on for certain predictions, if it does.
 - How does everything look for the multimodal approach, does it make sense or not?
And ways to improve the model

\printbibliography
